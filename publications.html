---
layout: default
---

<div class="row">
<div class="col-12 col-lg-4">
      
    <h1 style="margin-top:0">Publications</h1>
            
</div>
        
<div class="col-12 col-lg-8">

<div class="pub-list-item" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>
  
  <span class="article-metadata li-cite-author">
    
  <span style="font-weight:bold">
      <a>Hao Yen</a></span>, <span >
      <a>Pin-Jui Ku</a></span>, <span >
      <a>Chao-Han Huck Yang</a></span>, <span >
      <a>Hu hu</a></span>, <span >
      <a>Sabato Marco Siniscalchi</a></span>, <span >
      <a>Pin-Yu Chen</a></span>, <span >
      <a>Yu Tsao</a></span>.
  </span>
  <a href="https://arxiv.org/abs/2110.03894">Neural Model Reprogramming with Similarity Based Mapping for Low-Resource Spoken Command Recognition</a>.
  <em>Interspeech 2023</em>
  
  <details>
  <summary>Details...</summary>
  <div style="border:1px solid #e3e3e3; background-color: #f5f5f5; box-shadow: inset 0 1px 1px rgba(0,0,0,.05); min-height: 20px; padding:1rem">
      <h5><strong>Abstract</strong></h5>
      <p style="font-size: 1rem; margin-bottom:0">
     In this study, we propose a novel adversarial reprogramming (AR) approach for low-resource spoken command recognition (SCR), and build an AR-SCR system. 
     The AR procedure aims to modify the acoustic signals (from the target domain) to repurpose a pretrained SCR model (from the source domain). 
     To solve the label mismatches between source and target domains, and further improve the stability of AR, we propose a novel similarity-based label mapping technique to align classes. 
     In addition, the transfer learning (TL) technique is combined with the original AR process to improve the model adaptation capability. 
     We evaluate the proposed AR-SCR system on three low-resource SCR datasets, including Arabic, Lithuanian, and dysarthric Mandarin speech. 
     Experimental results show that with a pretrained AM trained on a large-scale English dataset, the proposed AR-SCR system outperforms the current <strong>state-of-the-art</strong> results on Arabic and Lithuanian speech commands datasets, with only a limited amount of training data.</p></div>
  </details>
  
</div>

<div class="pub-list-item" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>
  
  <span class="article-metadata li-cite-author">
    
  <span style="font-weight:bold">
      <a>Hao Yen</a></span>, <span >
      <a>Fran√ßois G. Germain</a></span>, <span >
      <a>Gordon Wichern</a></span>, <span >
      <a>Jonathan Le Roux</a></span>.
  </span>
  <a href="https://arxiv.org/abs/2011.01691">Cold Diffusion for Speech Enhancement</a>.
  <em>IEEE ICASSP 2023 Oral</em>
  
  <details>
  <summary>Details...</summary>
  <div style="border:1px solid #e3e3e3; background-color: #f5f5f5; box-shadow: inset 0 1px 1px rgba(0,0,0,.05); min-height: 20px; padding:1rem">
      <h5><strong>Abstract</strong></h5>
      <p style="font-size: 1rem; margin-bottom:0">
      Diffusion models have recently shown promising results for difficult enhancement tasks such as the conditional and unconditional restoration of natural images and audio signals. 
      In this work, we explore the possibility of leveraging a recently proposed advanced iterative diffusion model, namely cold diffusion, to recover clean speech signals from noisy signals. 
      The unique mathematical properties of the sampling process from cold diffusion could be utilized to restore high-quality samples from arbitrary degradations. 
      Based on these properties, we propose an improved training algorithm and objective to help the model generalize better during the sampling process. 
      We verify our proposed framework by investigating two model architectures. 
      Experimental results on benchmark speech enhancement dataset VoiceBank-DEMAND demonstrate the strong performance of the proposed approach compared to representative discriminative models and diffusion-based enhancement models.</p></div>
  </details>
  
</div>

<div class="pub-list-item" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>
  
  <span class="article-metadata li-cite-author">
    
  <span style="font-weight:bold">
      <a>Hao Yen</a></span>, <span >
      <a>Woojay Jeon</a></span>.
  </span>
  <a href="https://arxiv.org/abs/2210.16726">Improvements to Embedding-Matching Acoustic-to-Word ASR Using Multiple-Hypothesis Pronunciation-Based Embeddings</a>.
  <em>IEEE ICASSP 2023</em>
      
  <details>
  <summary>Details...</summary>
  <div style="border:1px solid #e3e3e3; background-color: #f5f5f5; box-shadow: inset 0 1px 1px rgba(0,0,0,.05); min-height: 20px; padding:1rem">
      <h5><strong>Abstract</strong></h5>
      <p style="font-size: 1rem; margin-bottom:0">
      In embedding-matching acoustic-to-word (A2W) ASR, every word in the vocabulary is represented by a fixed-dimension embedding vector that can be added or removed independently of the rest of the system. 
      The approach is potentially an elegant solution for the dynamic out-of-vocabulary (OOV) words problem, where speaker- and context-dependent named entities like contact names must be incorporated into the ASR on-the-fly for every speech utterance at testing time. 
      Challenges still remain, however, in improving the overall accuracy of embedding-matching A2W. 
      In this paper, we contribute two methods that improve the accuracy of embedding-matching A2W. 
      First, we propose internally producing multiple embeddings, instead of a single embedding, at each instance in time, which allows the A2W model to propose a richer set of hypotheses over multiple time segments in the audio. 
      Second, we propose using word pronunciation embeddings rather than word orthography embeddings to reduce ambiguities introduced by words that have more than one sound. 
      We show that the above ideas give significant accuracy improvement, with the same training data and nearly identical model size, in scenarios where dynamic OOV words play a crucial role. 
      On a dataset of queries to a speech-based digital assistant that include many user-dependent contact names, we observe up to 18% decrease in word error rate using the proposed improvements.</p></div>
  </details>
  
</div>
    
<div class="pub-list-item" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>
  
  <span class="article-metadata li-cite-author">
    
  <span style="font-weight:bold">
      <a>Hao Yen</a></span>, <span >
      <a>Chao-Han Huck Yang</span>, <span >
      <a>Hu Hu</span>, <span >
      <a>Sabato Marco Siniscalchi</span>, <span >
      <a>Qing Wang</span>, <span >
      <a>Yuyang Wang</span>, <span >
      <a>Xianjun Xia</span>, <span >
      <a>Yuanjun Zhao</span>, <span >
      <a>Yuzhong Wu</span>, <span >
      <a>Yannan Wang</span>, <span >
      <a>Jun Du</span>, <span >
      <a>Chin-Hui Lee</a></span>.
  </span>
  <a href="https://arxiv.org/abs/2107.01461">A Lottery Ticket Hypothesis Framework for Low-Complexity Device-Robust Neural Acoustic Scene Classification</a>.
  <em>DCASE 2021</em>
        
  <details>
  <summary>Details...</summary>
  <div style="border:1px solid #e3e3e3; background-color: #f5f5f5; box-shadow: inset 0 1px 1px rgba(0,0,0,.05); min-height: 20px; padding:1rem">
      <h5><strong>Abstract</strong></h5>
      <p style="font-size: 1rem; margin-bottom:0">
      We propose a novel neural model compression strategy combining data augmentation, knowledge transfer, pruning, and quantization for device-robust acoustic scene classification (ASC). 
      Specifically, we tackle the ASC task in a low-resource environment leveraging a recently proposed advanced neural network pruning mechanism, namely Lottery Ticket Hypothesis (LTH), to find a sub-network neural model associated with a small amount non-zero model parameters. 
      The effectiveness of LTH for low-complexity acoustic modeling is assessed by investigating various data augmentation and compression schemes, and we report an efficient joint framework for low-complexity multi-device ASC, called <em>Acoustic Lottery</em>. 
      Acoustic Lottery could compress an ASC model up to 1/104 and attain a superior performance (validation accuracy of 79.4% and Log loss of 0.64) compared to its not compressed seed model. 
      All results reported in this work are based on a joint effort of four groups, namely GT-USTC-UKE-Tencent, aiming to address the "Low-Complexity Acoustic Scene Classification (ASC) with Multiple Devices" in the DCASE 2021 Challenge Task 1a.</p></div>
  </details>
  
</div>
    
<div class="pub-list-item" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>
  
  <span class="article-metadata li-cite-author">
    
  <span style="font-weight:bold">
      <a>Hao Yen</a></span>, <span >
      <a>Ppin-Jui Ku</span>, <span >
      <a>Ming-Chi Yen</span>, <span >
      <a>Hung-Shin Lee</span>, <span >
      <a>Hsin-Min Wang</a></span>.
  </span>
  <a href="http://dcase.community/documents/workshop2020/proceedings/DCASE2020Workshop_Yen_47.pdf">Joint Training of Guided Learning and Mean Teacher Models for Sound Event Detection</a>.
  <em>DCASE 2020</em>
  
</div>
    
</div>

    
</div>
